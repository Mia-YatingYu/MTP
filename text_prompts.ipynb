{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-09T10:31:47.041395400Z",
     "start_time": "2025-02-09T10:31:45.425421600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/yuyating/workspace/STDD'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import clip\n",
    "import torch\n",
    "root = '/home/yuyating/workspace/STDD'\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_templates(dataset_name):\n",
    "    label_file = root + f\"/zs_label_db/classes_label_{dataset_name}.yml\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    # classes = data['classes']  # list: ['brush hair', ...]\n",
    "    # templates = data['templates']  # list: ['a video of a person {}.', ...]\n",
    "    # obj_templates = data['obj_templates']\n",
    "    return data\n",
    "data = get_templates('ucf101')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T10:31:48.288342500Z",
     "start_time": "2025-02-09T10:31:48.257616400Z"
    }
   },
   "id": "a7f37aed748d5d4d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_xprompt(dataset_name):\n",
    "    xprompt_file = f\"data/{dataset_name}/classes_xprompt_{dataset_name}.yml\"\n",
    "    with open(xprompt_file, 'r') as f:\n",
    "        text = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T10:31:55.190117200Z",
     "start_time": "2025-02-09T10:31:55.173542400Z"
    }
   },
   "id": "6d59ce03b021286a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def expand_cls_tokenized(cls_tokenized, num_prompt):\n",
    "    expanded_cls_tokenized = []\n",
    "    for cls in cls_tokenized:\n",
    "        cur_size = cls.size(0)\n",
    "        if cur_size < num_prompt:\n",
    "            repeats = num_prompt // cur_size\n",
    "            expanded_cls = torch.cat([cls] * repeats, dim=0)\n",
    "            remaining = num_prompt % cur_size\n",
    "            if remaining > 0:\n",
    "                expanded_cls = torch.cat([expanded_cls, cls[:remaining]], dim=0)\n",
    "\n",
    "        else:\n",
    "            expanded_cls = cls[:num_prompt]\n",
    "        expanded_cls_tokenized.append(expanded_cls)\n",
    "    expanded_cls_tokenized = torch.stack(expanded_cls_tokenized, dim=0)\n",
    "    return expanded_cls_tokenized\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T10:31:59.154456200Z",
     "start_time": "2025-02-09T10:31:59.121066300Z"
    }
   },
   "id": "7e5334ae55fa4cf6",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def expand_cls_text(cls_text_list):\n",
    "    min_prompt = min([len(i) for i in cls_text_list])\n",
    "    max_prompt = max([len(i) for i in cls_text_list])\n",
    "\n",
    "    expanded_cls_text_list = [sublist + [sublist[i % len(sublist)] for i in range(max_prompt-len(sublist))] for sublist in cls_text_list]\n",
    "    return expanded_cls_text_list, min_prompt, max_prompt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T10:32:01.198844900Z",
     "start_time": "2025-02-09T10:32:01.187830Z"
    }
   },
   "id": "6d2988c70d55a67a",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def text_prompt(data, dataset: str, num_templates: int, cls_prompt_type: str):\n",
    "    text = get_xprompt(dataset)\n",
    "    templates = get_templates(dataset)['templates'] # k: classes, templates, obj_templates\n",
    "    classes = [i[1] for i in data.classes] # [\"c1\", \"c2\", ...]\n",
    "    num_classes = len(classes)\n",
    "    n_prompts = [0, 0]\n",
    "\n",
    "    total_templates = len(templates)\n",
    "    num_templates = min(num_templates, total_templates)\n",
    "    templates = templates[:num_templates] # a video of a person {}.\n",
    "\n",
    "    tokenized_dict = {}\n",
    "    cls_text_dict = {}\n",
    "    text_dict = {}\n",
    "    xoo_dict = {} # {0: [xprompt_oo,...], 1: [xprompt_oo], ...}\n",
    "    xao_dict = {} # {0: [xprompt_ao,...], 1: [xprompt_ao], ...}\n",
    "    xaa_dict = {} # {0: [xprompt_aa,...], 1: [xprompt_aa], ...}\n",
    "    for i, t in enumerate(text.values()):\n",
    "        xoo_dict[i] = t['xprompt_oo']\n",
    "        xao_dict[i] = t['xprompt_ao']\n",
    "        xaa_dict[i] = t['xprompt_aa']\n",
    "    for ii, txt in enumerate(templates):\n",
    "        text_dict[ii] = {\n",
    "            'a': [[txt.format(c)] for i, c in enumerate(classes)],\n",
    "            'xoo': [],\n",
    "            'xao': [],\n",
    "            'xaa': []\n",
    "        }\n",
    "        tokenized_dict[ii] = []\n",
    "        for i, c in enumerate(classes):\n",
    "            ci_xoo_list = text_dict[ii]['a'][i][:] # [\"a {ci}.\"]\n",
    "            ci_xao_list = text_dict[ii]['a'][i][:]\n",
    "            ci_xaa_list = text_dict[ii]['a'][i][:]\n",
    "            for j, t in enumerate(xoo_dict[i]):\n",
    "                ci_xoo_list.append(txt.format(f\"{c}, {t}\")) # [\"a video of a person brush hair, where hair...\"]\n",
    "            text_dict[ii]['xoo'].append(ci_xoo_list)\n",
    "            for j, t in enumerate(xao_dict[i]):\n",
    "                ci_xao_list.append(txt.format(f\"{c}, {t}\"))\n",
    "            text_dict[ii]['xao'].append(ci_xao_list)\n",
    "            for j, t in enumerate(xaa_dict[i]):\n",
    "                ci_xaa_list.append(txt.format(f\"{c}, {t}\"))\n",
    "            text_dict[ii]['xaa'].append(ci_xaa_list)\n",
    "        if cls_prompt_type == 'xoo':\n",
    "            cls_text_dict[ii] = text_dict[ii]['xoo']\n",
    "        elif cls_prompt_type == 'xao':\n",
    "            cls_text_dict[ii] = text_dict[ii]['xao']\n",
    "        elif cls_prompt_type == 'xaa':\n",
    "            cls_text_dict[ii] = text_dict[ii]['xaa']\n",
    "        elif cls_prompt_type == 'xmix':\n",
    "            cls_text_dict[ii] = []\n",
    "            for i in range(len(classes)):\n",
    "                cls_text_dict[ii].append(list(set(text_dict[ii]['xoo'][i] + text_dict[ii]['xao'][i] + text_dict[ii]['xaa'][i])))\n",
    "        else:\n",
    "            cls_text_dict[ii] = text_dict[ii]['a']\n",
    "\n",
    "\n",
    "        cls_text_dict[ii], n_prompts[0], n_prompts[1] = expand_cls_text(cls_text_dict[ii])\n",
    "\n",
    "        for n in range(n_prompts[1]):\n",
    "            tokenized_dict[ii].append(torch.cat([clip.tokenize(t[n]) for t in cls_text_dict[ii]])) # [c 77, c 77,...]\n",
    "\n",
    "        tokenized_dict[ii] = torch.cat(tokenized_dict[ii])\n",
    "        # for i in range(len(cls_text_dict[ii])):\n",
    "        #     tokenized_dict[ii].append(torch.cat([clip.tokenize(t) for t in cls_text_dict[ii][i]]))\n",
    "\n",
    "\n",
    "    # cls_tokenized = [torch.cat([tokenized_dict[i][j] for i in range(num_templates)], dim=0) for j in range(num_classes)]\n",
    "\n",
    "    cls_tokenized = torch.cat([v for v in tokenized_dict.values()]) # (num_templates max_prompt num_cls) 77\n",
    "\n",
    "\n",
    "    # expand and repeat cls_tokenized dim to max\n",
    "    # cls_tokenized = expand_cls_tokenized(cls_tokenized, max_prompt) # c max 77\n",
    "    return cls_tokenized, cls_text_dict, tokenized_dict, num_templates, n_prompts\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T10:32:06.715296300Z",
     "start_time": "2025-02-09T10:32:06.661385900Z"
    }
   },
   "id": "d9eae190fb121eb",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_en_labels(en_list, en_dict):\n",
    "    # classes = [i[1] for i in data.classes] # [\"c1\", \"c2\", ...]\n",
    "    # en_list, en_dict = get_ASKG_entity(dataset, classes, entity_type) # ['e1', 'e2', ...], {0:['','',...], 1:[], ...}\n",
    "    en_label_dict = {}\n",
    "    en_label_list = []\n",
    "    for c, en_li in en_dict.items():\n",
    "        en_labels = []\n",
    "        for i, en in enumerate(en_list):\n",
    "            if en in en_li:\n",
    "                en_labels.append(i)\n",
    "        en_label_dict[c] = en_labels\n",
    "        en_label_list.append(en_labels)\n",
    "    return en_label_dict, en_label_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T10:32:08.497751400Z",
     "start_time": "2025-02-09T10:32:08.467643200Z"
    }
   },
   "id": "fc2cabf486656fdf",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6727b98a8314b3b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
